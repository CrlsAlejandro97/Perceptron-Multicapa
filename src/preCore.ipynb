{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3637cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import function as f\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d8d5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializamos los pesos de la Capa 1\n",
    "w = np.random.random(500)\n",
    "w11 = w[:100]\n",
    "w12 = w[100:200]\n",
    "w13 = w[200:300]\n",
    "w14 = w[300:400]\n",
    "w15 = w[400:500]\n",
    "W1=[w11,w12,w13,w14,w15]\n",
    "\n",
    "#Inicializamos los pesos de la Capa 2\n",
    "w = np.random.random(25)\n",
    "w21 = w[:5]\n",
    "w22 = w[5:10]\n",
    "w23 = w[10:15]\n",
    "w24 = w[15:20]\n",
    "w25 = w[20:25]\n",
    "W2 = [w21,w22,w23,w24,w25] \n",
    "\n",
    "#Incializamos los pesos de la Capa 3\n",
    "w = np.random.random(15)\n",
    "w31 = w[:5]\n",
    "w32 = w[5:10]\n",
    "w33 = w[10:15]\n",
    "W3 = [w31,w32,w33]\n",
    "\n",
    "#Bias\n",
    "b = random.random(13)\n",
    "\n",
    "b1 = b[:5]\n",
    "b2 = b[5:10]\n",
    "b3 = b[10:]\n",
    "\n",
    "#Factor de aprendizaje\n",
    "alfa = random.random()\n",
    "\n",
    "#Codificacion de clases\n",
    "#b = 100\n",
    "#d = 010\n",
    "#f = 001\n",
    "\n",
    "Ye =[{'b':[1,0,0]},{'d':[0,1,0]},{'f':[0,0,1]}]\n",
    "sizes = [100,5,5,3]\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06703312",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (pd.read_csv(os.path.join(os.path.abspath(''),\"data\",\"distorsionadas\",'100','letras.csv'),sep=';',header=None)).to_numpy()\n",
    "#Convertimos el dataframe en array numpy\n",
    "data_train = data[:int(len(data)*0.8)]\n",
    "data_test = data[int(len(data)*0.8)+1:int(len(data)*0.8)+int(len(data)*0.15)]\n",
    "data_validation = data[int(len(data)*0.8)+int(len(data)*0.15)+1:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e68562a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "#Dividiendo data train en una tupla (entrada,clase)\n",
    "letras_train = []\n",
    "for letra in data_train:\n",
    "    x_train = letra[:100]\n",
    "    y_train = letra[100:]\n",
    "    letras_train.append((x_train, y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c83af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    grad_b = [np.zeros(b.shape) for b in biases]\n",
    "    grab_w = [np.zeros(w.shape) for w in weights]\n",
    "\n",
    "    for j in range(len(data_train)):\n",
    "        for x,y in (letras_train[i][0],letras_train[i][1]):\n",
    "        \n",
    "            ## Aqui se aplica feedforward\n",
    "            activacion = x\n",
    "            activaciones = [x] # almacenar todas las activaciones\n",
    "            zs = [] ## alamacenar todos los vectores z\n",
    "            for bias,weight in zip(biases,weights):\n",
    "                z = np.dot(weight,activacion)+b\n",
    "                zs.append(z)\n",
    "                    \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "\n",
    "    ##--------------------------_##\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        weights = [w-(lr)*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        biases = [b-(lr)*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2a880c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos anteriores W3:  [array([0.34454629, 0.3475878 , 0.73239798, 0.84657507, 0.06655497]), array([0.67860019, 0.64487817, 0.95971476, 0.24966497, 0.31261677]), array([0.79082731, 0.34903601, 0.53398945, 0.40589658, 0.47137842])]\n",
      "\n",
      "\n",
      "Nuevos pesos de W3:  [array([0.32528449, 0.32832599, 0.71313618, 0.82731326, 0.04729317]), array([0.67894258, 0.64522056, 0.96005715, 0.25000736, 0.31295916]), array([0.77486226, 0.33307096, 0.51802439, 0.38993152, 0.45541337])]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "\n",
    "    #Primer capa\n",
    "    ys1 =[]\n",
    "    for j in range(len(W1)):\n",
    "        #Suma ponderada\n",
    "        Z = np.dot(letras_train[i][0],W1[j])\n",
    "        #Funcion de activacion\n",
    "        ys1.append(f.lineal(Z+b[j]))\n",
    "    #print(\"Primer capa: \", ys1)\n",
    "\n",
    "    #Segunda capa\n",
    "    ys2 = []\n",
    "    for j in range(len(W2)):\n",
    "        #Suma ponderada\n",
    "        Z = np.dot(ys1,W2[j])\n",
    "        #Funcion de activacion\n",
    "        ys2.append(f.lineal(Z+b[j]))\n",
    "    #print(\"2da capa: \", ys2)\n",
    "    #Capa de salida\n",
    "    ys3 = []\n",
    "    for j in range(len(W3)):\n",
    "        #Suma ponderada\n",
    "        Z = np.dot(ys2,W3[j])\n",
    "        #Funcion de activacion\n",
    "        ys3.append(f.sigmoide(Z+b[j]))\n",
    "    #print(\"Capa salida: \", ys3)\n",
    "    #!!-----Calculo del error-----!!!\n",
    "    \n",
    "    error = []\n",
    "    for j in range(len(ys3)):\n",
    "        # Salida espera - Salida obtenida\n",
    "        error.append(letras_train[i][1][j] - ys3[j])\n",
    "    #!!-------Backpropagation-----!!\n",
    "    #Actualizacion W3\n",
    "    new_weights = []\n",
    "    ys2 = np.array(ys2)\n",
    "    ys3 = np.array(ys3)\n",
    "    for j in range (len(W3)):\n",
    "        new_weights.append(W3[j]+alfa*derivate_sigmoide(ys3[j])*error[j]*ys2)\n",
    "    print(\"Pesos anteriores W3: \", W3)\n",
    "    print(\"\\n\")\n",
    "    print(\"Nuevos pesos de W3: \",new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423b6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
