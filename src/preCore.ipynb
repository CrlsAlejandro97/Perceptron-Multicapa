{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3637cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from function import lineal, sigmoide, derivate_sigmoide, derivate_lineal,Backpropagation, derivate_error,Gradientdescent\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8d5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializamos los pesos de la Capa 1\n",
    "w = np.random.random(500)\n",
    "w11 = w[:100]\n",
    "w12 = w[100:200]\n",
    "w13 = w[200:300]\n",
    "w14 = w[300:400]\n",
    "w15 = w[400:500]\n",
    "W1=[w11,w12,w13,w14,w15]\n",
    "#Inicializamos los pesos anteriores\n",
    "w = np.zeros(500)\n",
    "w11ant = w[:100]\n",
    "w12ant = w[100:200]\n",
    "w13ant = w[200:300]\n",
    "w14ant = w[300:400]\n",
    "w15ant = w[400:500]\n",
    "W1ant = [w11ant,w12ant,w13ant,w14ant,w15ant]\n",
    "\n",
    "#Inicializamos los pesos de la Capa 2\n",
    "w = np.random.random(25)\n",
    "w21 = w[:5]\n",
    "w22 = w[5:10]\n",
    "w23 = w[10:15]\n",
    "w24 = w[15:20]\n",
    "w25 = w[20:25]\n",
    "W2 = [w21,w22,w23,w24,w25] \n",
    "#Inicializamos los pesos anteriores\n",
    "w = np.zeros(25)\n",
    "w21ant = w[:5]\n",
    "w22ant = w[5:10]\n",
    "w23ant = w[10:15]\n",
    "w24ant = w[15:20]\n",
    "w25ant = w[20:25]\n",
    "W2ant = [w21ant,w22ant,w23ant,w24ant,w25ant]\n",
    "\n",
    "#Incializamos los pesos de la Capa 3\n",
    "w = np.random.random(15)\n",
    "w31 = w[:5]\n",
    "w32 = w[5:10]\n",
    "w33 = w[10:15]\n",
    "W3 = [w31,w32,w33]\n",
    "#Inicializamos los pesos anteriores\n",
    "w = np.zeros(15)\n",
    "w31ant = w[:5]\n",
    "w32ant = w[5:10]\n",
    "w33ant = w[10:15]\n",
    "W3ant = [w31ant,w32ant,w33ant]\n",
    "#Conversiones\n",
    "\n",
    "W1 = np.array(W1)\n",
    "W2 = np.array(W2)\n",
    "W3 = np.array(W3)\n",
    "W = [W1,W2,W3]\n",
    "\n",
    "W1ant = np.array(W1ant)\n",
    "W2ant = np.array(W2ant)\n",
    "W3ant = np.array(W3ant)\n",
    "Want = [W1ant,W2ant,W3ant]\n",
    "\n",
    "#Bias\n",
    "b = random.random()\n",
    "\n",
    "#Factor de aprendizaje\n",
    "alfa = np.array(random.random())\n",
    "beta = np.array(random.random())\n",
    "\n",
    "#Codificacion de clases\n",
    "#b = 100\n",
    "#d = 010\n",
    "#f = 001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06703312",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (pd.read_csv(os.path.join(os.path.abspath(''),\"data\",\"distorsionadas\",'100','letras.csv'),sep=';',header=None)).to_numpy()\n",
    "#Convertimos el dataframe en array numpy\n",
    "data_train = data[:int(len(data)*0.8)]\n",
    "data_test = data[int(len(data)*0.8)+1:int(len(data)*0.8)+int(len(data)*0.15)]\n",
    "data_validation = data[int(len(data)*0.8)+int(len(data)*0.15)+1:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e68562a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividiendo data train en una tupla (entrada,clase)\n",
    "letras_train = []\n",
    "for letra in data_train:\n",
    "    x_test = letra[:100]\n",
    "    y_test = letra[100:]\n",
    "    letras_train.append((x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2a880c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76436713 0.90421145 0.49100184 0.20301242 0.38724298]\n",
      " [0.59061022 0.43181977 0.36213616 0.6590047  0.06262891]\n",
      " [0.36064391 0.74763696 0.46101395 0.14692194 0.01542044]]\n"
     ]
    }
   ],
   "source": [
    "print(W[2])\n",
    "for i in range(1):\n",
    "\n",
    "    #Primer capa\n",
    "    ys1 =[]\n",
    "    for j in range(len(W1)):\n",
    "        #Suma ponderada\n",
    "        Z = np.dot(letras_train[i][0],W1[j])\n",
    "        #Funcion de activacion\n",
    "        ys1.append(lineal(Z+b))\n",
    "    #print(\"Primer capa: \", ys1)\n",
    "\n",
    "    #Segunda capa\n",
    "    ys2 = []\n",
    "    for j in range(len(W2)):\n",
    "        #Suma ponderada\n",
    "        Z = np.dot(ys1,W2[j])\n",
    "        #Funcion de activacion\n",
    "        ys2.append(lineal(Z+b))\n",
    "    #print(\"2da capa: \", ys2)\n",
    "    #Capa de salida\n",
    "    ys3 = []\n",
    "    for j in range(len(W3)):\n",
    "        #Suma ponderada\n",
    "        Z = np.dot(ys2,W3[j])\n",
    "        #Funcion de activacion\n",
    "        ys3.append(sigmoide(Z+b))\n",
    "    #print(\"Capa salida: \", ys3)\n",
    "\n",
    "    #!!-------Backpropagation-----!!\n",
    "    ys1 = np.array(ys1)\n",
    "    ys2 = np.array(ys2)\n",
    "    ys3 = np.array(ys3)\n",
    "    activations = [ys1,ys2,ys3]\n",
    "    ye = letras_train[i][1]\n",
    "    deltas = Backpropagation(ye,activations,W[1:])\n",
    "    #!!----- Gradient descent------!!\n",
    "    Want, W = Gradientdescent(deltas,W,Want,alfa,beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc9612ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.18346059 1.38441923 0.79063009 0.37678445 0.64152686]\n",
      " [0.80935563 0.5811711  0.48103458 0.90763965 0.05063764]\n",
      " [0.60480332 1.16091883 0.74903675 0.29768126 0.1087114 ]]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(W[2])\n",
    "print(W2ant[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## PREDICT \n",
    "letra = [\"b\",\"d\",\"f\"]\n",
    "porcentaje = 0\n",
    "for i in range(len(1)):\n",
    "    ys1 =[]\n",
    "    z1 = []\n",
    "    for j in range(len(w1)):\n",
    "                #Suma ponderada\n",
    "        Z1 = np.dot(letras_train[i][0],w1[j])\n",
    "                #Funcion de activacion\n",
    "        ys1.append(lineal(Z1+b1[j]))\n",
    "        z1.append(Z1)\n",
    "                \n",
    "    z1 = np.array(z1)\n",
    "    ys1 = np.array(ys1)\n",
    "\n",
    "    #print(\"Primer capa: \", ys1)\n",
    "    #Segunda capa\n",
    "    ys2 = []\n",
    "    z2 = []\n",
    "    for j in range(len(w2)):\n",
    "        #Suma ponderada\n",
    "        Z2 = np.dot(ys1,w2[j])\n",
    "        #Funcion de activacion\n",
    "        ys2.append(f.lineal(Z2+b2[j]))\n",
    "        z2.append(Z2)\n",
    "                \n",
    "    z2 = np.array(z2)\n",
    "    ys2 = np.array(ys2)\n",
    "\n",
    "    #print(\"2da capa: \", ys2)\n",
    "    #Capa de salida\n",
    "    ys3 = []\n",
    "    z3 = []\n",
    "    for j in range(len(w3)):\n",
    "        #Suma ponderada\n",
    "        Z3 = np.dot(ys2,w3[j])\n",
    "        #Funcion de activacion\n",
    "        ys3.append(f.sigmoide(Z3+b3[j]))\n",
    "        z3.append(Z3)\n",
    "                #print(\"Capa salida: \", ys3)\n",
    "                #!!-----Calculo del error-----!!!\n",
    "\n",
    "    ys3 = np.array(ys3)\n",
    "    z3 = np.array(z3)\n",
    "\n",
    "    if(np.argmax(ys3) == np.argmax(letras_train[i][1])):\n",
    "        porcentaje += 1\n",
    "    print(\"Prediccion: {} letra: {} ---- valor real: {}, letra: {}\".format( np.argmax(ys3)+1,letra[ np.argmax(ys3)],np.argmax(data_test_predict[i][1])+1,letra[np.argmax(data_test_predict[i][1])] ))\n",
    "print(\"Porcentaje predicho: {}%\".format((porcentaje/len(letras_train))*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba23b61cdc6890bf9b74f0059fdc1fca5173150fee92ba939d36054f0efba838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
