{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from functionsMaths import calculateDelta, get_mse,gradiente_descendente, feedforward, div_tuplas, init_params_test\n",
    "import os\n",
    "import math\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cantidad de letras por cada set\n",
    "cantidad =['100','500','1000']\n",
    "\n",
    "#Porcentajes datos de validacion\n",
    "set_validation = [0.1,0.2,0.3]\n",
    "\n",
    "#Cantidad de epocas\n",
    "epocas = 1\n",
    "\n",
    "#Pruebas solicitadas\n",
    "#1er elemento corresponde a cantidad de neuronas por capas\n",
    "    #[5] = 5 neuronas con 1 capa oculta \n",
    "    #[5,5] = 5 neuronas con 2 capas ocultas\n",
    "#2do porcentaje de aprendizaje\n",
    "#3ro porcentaje de momento\n",
    "test =[ [[5],0.5,0.5], [[10],0.5,0.5], [[5,5],0.5,0.5], [[10,10],0.5,0.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for nro in range(len(test)):\n",
    "    #architecture\n",
    "    W,B = init_params_test(test[nro][0])\n",
    "    lr = np.array(test[nro][1])\n",
    "    m = np.array(test[nro][2])\n",
    "    Y = []\n",
    "    for set in range (len(cantidad)):\n",
    "        for percentage in range(len(set_validation)):\n",
    "        \n",
    "            data = (pd.read_csv(os.path.join(os.path.abspath(''),\"data\",\"distorsionadas\",cantidad[set],'letras.csv'),sep=';',header=None)).to_numpy()\n",
    "            #Convertimos el dataframe en array numpy\n",
    "            data_train = data[:int(len(data)-int(len(data)*set_validation[percentage])*2)]\n",
    "            data_validation = data[int(len(data)-int(len(data)*set_validation[percentage])*2):int(len(data)-int(len(data)*set_validation[percentage])*2)+int(len(data)*set_validation[percentage])]\n",
    "            data_test = data[int(len(data)-int(len(data)*set_validation[percentage])*2)+int(len(data)*set_validation[percentage]):]\n",
    "\n",
    "            #Division de datos en tuplas(datos,clase)\n",
    "            letras_train = div_tuplas(data_train)\n",
    "            letras_test = div_tuplas(data_train)\n",
    "            letras_validation = div_tuplas(data_validation)\n",
    "\n",
    "\n",
    "            #Comienzo de Perceptron\n",
    "            err_train_epoc = 0\n",
    "            err_valid_epoc = 0\n",
    "\n",
    "            for epoc in range(epocas):\n",
    "                err_training = np.array([0,0,0])\n",
    "                err_validation = np.array([0,0,0])\n",
    "\n",
    "                #TRAINING\n",
    "                for i in range(len(letras_train)):\n",
    "        \n",
    "                    Y = feedforward(letras_train[i][0],W,B,len(test[nro][0])+2)\n",
    "                    Ye = np.array(letras_train[i][1])\n",
    "                    err_training = err_training + ((Ye-Y[-1])**2)\n",
    "\n",
    "                    #Deltas\n",
    "                    deltas = calculateDelta(Y[-1],Ye,W[1:])\n",
    "\n",
    "                    W,B = gradiente_descendente(W,B,deltas,Y[0:len(Y)-1],lr,m)\n",
    "\n",
    "                #VALIDATION\n",
    "                for i in range(len(letras_validation)):\n",
    "                    Y = feedforward(letras_validation[i][0],W,B,len(test[nro][0])+2)\n",
    "                    Ye = np.array(letras_validation[i][1])\n",
    "                err_validation = err_validation + ((Ye-Y[-1])**2)\n",
    "\n",
    "            err_train_epoc = get_mse(np.mean(err_training),len(letras_train))\n",
    "            err_valid_epoc = get_mse(np.mean(err_validation),len(letras_validation))\n",
    "            rows.append([len(test[nro][0]),test[nro][0][0],epoc,cantidad[set],str(set_validation[percentage]*100)+' %',err_train_epoc,err_valid_epoc])\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No axis named count neuron for object type DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:550\u001b[0m, in \u001b[0;36mNDFrame._get_axis_number\u001b[1;34m(cls, axis)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_AXIS_TO_AXIS_NUMBER[axis]\n\u001b[0;32m    551\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'count neuron'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aleoc\\OneDrive\\Escritorio\\Perceptron-Multicapa\\src\\Test.ipynb Celda 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aleoc/OneDrive/Escritorio/Perceptron-Multicapa/src/Test.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tabla \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(rows,columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcount layer hidden\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcount neuron\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mepoc\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcount data\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mpercentage validation\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39merror training\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39merror validation\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aleoc/OneDrive/Escritorio/Perceptron-Multicapa/src/Test.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tabla\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39mepoc\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mcount neuron\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:7713\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   7711\u001b[0m \u001b[39mif\u001b[39;00m level \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m by \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   7712\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7713\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_axis_number(axis)\n\u001b[0;32m   7715\u001b[0m \u001b[39m# https://github.com/python/mypy/issues/7642\u001b[39;00m\n\u001b[0;32m   7716\u001b[0m \u001b[39m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[39;00m\n\u001b[0;32m   7717\u001b[0m \u001b[39m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[39;00m\n\u001b[0;32m   7718\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   7719\u001b[0m     obj\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   7720\u001b[0m     keys\u001b[39m=\u001b[39mby,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   7728\u001b[0m     dropna\u001b[39m=\u001b[39mdropna,\n\u001b[0;32m   7729\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:552\u001b[0m, in \u001b[0;36mNDFrame._get_axis_number\u001b[1;34m(cls, axis)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_AXIS_TO_AXIS_NUMBER[axis]\n\u001b[0;32m    551\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m--> 552\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo axis named \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m for object type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: No axis named count neuron for object type DataFrame"
     ]
    }
   ],
   "source": [
    "tabla = pd.DataFrame(rows,columns=['count layer hidden','count neuron','epoc','count data','percentage validation','error training','error validation'])\n",
    "tabla.groupby('epoc','count neuron').sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba23b61cdc6890bf9b74f0059fdc1fca5173150fee92ba939d36054f0efba838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
