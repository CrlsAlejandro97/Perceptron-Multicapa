{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from functionsMaths import calculateDelta, get_mse,gradiente_descendente, feedforward, div_tuplas, init_params_test\n",
    "import os\n",
    "import math\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cantidad de letras por cada set\n",
    "cantidad =['100','500','1000']\n",
    "\n",
    "#Porcentajes datos de validacion\n",
    "set_validation = [0.1,0.2,0.3]\n",
    "\n",
    "#Cantidad de epocas\n",
    "epocas = 1\n",
    "\n",
    "#Pruebas solicitadas\n",
    "#1er elemento corresponde a cantidad de neuronas por capas\n",
    "    #[5] = 5 neuronas con 1 capa oculta \n",
    "    #[5,5] = 5 neuronas con 2 capas ocultas\n",
    "#2do porcentaje de aprendizaje\n",
    "#3ro porcentaje de momento\n",
    "test =[ [[5],0.5,0.5], [[10],0.5,0.5], [[5,5],0.5,0.5], [[10,10],0.5,0.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for nro in range(len(test)):\n",
    "    #architecture\n",
    "    W,B = init_params_test(test[nro][0])\n",
    "    lr = np.array(test[nro][1])\n",
    "    m = np.array(test[nro][2])\n",
    "    Y = []\n",
    "    for set in range (len(cantidad)):\n",
    "        for percentage in range(len(set_validation)):\n",
    "        \n",
    "            data = (pd.read_csv(os.path.join(os.path.abspath(''),\"data\",\"distorsionadas\",cantidad[set],'letras.csv'),sep=';',header=None)).to_numpy()\n",
    "            #Convertimos el dataframe en array numpy\n",
    "            data_train = data[:int(len(data)-int(len(data)*set_validation[percentage])*2)]\n",
    "            data_validation = data[int(len(data)-int(len(data)*set_validation[percentage])*2):int(len(data)-int(len(data)*set_validation[percentage])*2)+int(len(data)*set_validation[percentage])]\n",
    "            data_test = data[int(len(data)-int(len(data)*set_validation[percentage])*2)+int(len(data)*set_validation[percentage]):]\n",
    "\n",
    "            #Division de datos en tuplas(datos,clase)\n",
    "            letras_train = div_tuplas(data_train)\n",
    "            letras_test = div_tuplas(data_train)\n",
    "            letras_validation = div_tuplas(data_validation)\n",
    "\n",
    "\n",
    "            #Comienzo de Perceptron\n",
    "            err_train_epoc = 0\n",
    "            err_valid_epoc = 0\n",
    "\n",
    "            for epoc in range(epocas):\n",
    "                err_training = np.array([0,0,0])\n",
    "                err_validation = np.array([0,0,0])\n",
    "\n",
    "                #TRAINING\n",
    "                for i in range(len(letras_train)):\n",
    "        \n",
    "                    Y = feedforward(letras_train[i][0],W,B,len(test[nro][0])+2)\n",
    "                    Ye = np.array(letras_train[i][1])\n",
    "                    err_training = err_training + ((Ye-Y[-1])**2)\n",
    "\n",
    "                    #Deltas\n",
    "                    deltas = calculateDelta(Y[-1],Ye,W[1:])\n",
    "\n",
    "                    W,B = gradiente_descendente(W,B,deltas,Y[0:len(Y)-1],lr,m)\n",
    "\n",
    "                #VALIDATION\n",
    "                for i in range(len(letras_validation)):\n",
    "                    Y = feedforward(letras_validation[i][0],W,B,len(test[nro][0])+2)\n",
    "                    Ye = np.array(letras_validation[i][1])\n",
    "                err_validation = err_validation + ((Ye-Y[-1])**2)\n",
    "\n",
    "                err_train_epoc = get_mse(np.mean(err_training),len(letras_train))\n",
    "                err_valid_epoc = get_mse(np.mean(err_validation),len(letras_validation))\n",
    "                rows.append([len(test[nro][0]),test[nro][0][0],epoc,cantidad[set],str(set_validation[percentage]*100)+' %',round(err_train_epoc*100,4),round(err_valid_epoc*100,4)])\n",
    "        \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentage validation\n",
       "10.0 %    10.364283\n",
       "20.0 %     6.691500\n",
       "30.0 %     5.739283\n",
       "Name: percentage error training, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla = pd.DataFrame(rows,columns=['count layer hidden','count neuron','epoc','count data','percentage validation','percentage error training','percetage error validation'])\n",
    "tabla.groupby(by=[\"percentage validation\"])['percentage error training'].mean()\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla.plot(x='unemployment_rate', y='index_price', kind='scatter')\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla.to_csv('Pruebas.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba23b61cdc6890bf9b74f0059fdc1fca5173150fee92ba939d36054f0efba838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
